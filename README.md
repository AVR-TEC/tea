# Team TEA - Udacity / Didi entry
This is the code entry that got `0.3824329` score in round two of the Udacity / Didi competition.

### Dependencies
This obviously requires ROS. Please refer to the [ROS Documentation](http://wiki.ros.org/ROS/Tutorials/BuildingPackages) on how to build catkin packages.

Other dependencies:
* numpy 1.13.0
* [ROS Velodyne driver](https://bitbucket.org/DataspeedInc/ros_binaries)
* PCL 1.8 (may work with 1.7)
* [python-pcl bindings](https://github.com/duburlan/python-pcl)
* Keras 2.0.5
* Tensorflow 1.2
* HDF5

## Generating tracklets

The `tracklets` directory contains two convenience scripts for generating tracklets for car and pedestrian bags. As per the rules of the competition we used one single ros node which handles car and pedestrian bags via different command-line arguments: **for cars bags we used exactly the same parameters for all car bags**, as can be verified by looking at the XML comment in each tracklet:

```
<?xml version="1.0" encoding="UTF-8" standalone="yes" ?>
<!-- Generated with -sm ../scripts/torbusnet/lidarnet-car-r3-x13-d40-seg-rings_12_28-sectors_32-epoch37-val_class_acc0.9992.hdf5 -lm ../scripts/torbusnet/lidarnet-car-r2-fs-di-yawbalance-loc-rings_12_28-epoch10-val_loss0.0333.hdf5 -di -rfp -lpt 3 -bsw 1.2 -zc 0.06 -yc 0.075 -bsh 1.1 -b /home/antor/didi-ext/didi-data/release3/Data-points/testing/ford02.bag -->
<!DOCTYPE boost_serialization>
<boost_serialization signature="serialization::archive" version="9">
<tracklets class_id="0" tracking_level="0" version="0">
...
``` 

Before running either `cars.sh` or `ped.sh` make sure to:
* edit the full path of the location of the test bags, i.e. edit this path appropriately:

```
BAGPATH=/home/antor/didi-ext/didi-data/release3/Data-points/testing
``` 
* `roscore` has been started
* the pedestrian bag has the `/velodyne_points` topic (the original bag provided by Udacity did NOT contain it)

Both the `cars.sh` and `ped.sh` script just start our `ros_node.py` ros node with paths to the two model pairs we used (one model pair for the pedestrian and another for cars), as well as extra command-line arguments that control configurable hyper-parameters for the whole pipeline.

### Car tracklets

Car tracklets are generated by invoking the `car.sh` script as follows:

```
$ ./cars.sh \
../scripts/torbusnet/lidarnet-car-r3-x13-d40-seg-rings_12_28-sectors_32-epoch37-val_class_acc0.9992.hdf5 \
../scripts/torbusnet/lidarnet-car-r2-fs-di-yawbalance-loc-rings_12_28-epoch10-val_loss0.0333.hdf5 \
"-bsw 1.2 -zc 0.06 -yc 0.075 -bsh 1.1"
```

The tracklets for the final submission are in the `scripts/tracklets` directory and they contain XML comments with the exact arguments we used in the final submission. The organization can verify we used the same exact parameters for all car bags (for pedestrian bags this is a non-issue because there is only one bag). **Should other teams be allowed to fine-tune command-line arguments on a per-bag basis we would request Udacity our chance to do so to compete on equal conditions**. We strongly believe the spirit of the competition was to provide a general solution and not one that attempted to overfit on a per-bag basis.

### Pedestrian tracklet

Generate the pedestrian by invoking `ped.sh` script as follows:

```
$ ./ped.sh \
../scripts/torbusnet/lidarnet-ped-r3-x135-d40-seg-rings_12_28-sectors_32-epoch14-val_class_acc0.9990.hdf5 \
../scripts/torbusnet/lidarnet-ped-r3-d45-loc-rings_12_28-epoch01-val_loss0.0044.hdf5 \
"-yc 0.4 -pc -0.5 -zc -0.1"
```

## Performance

### Qualitative analysis

Here's a fragment of the visualization of the pedestrian test bag:

(click here for the full version)

and here's a fragment of visualizations for each of the car bags:

(click here for full version)

## Performance

The pipeline makes predictions in less that 100 msecs. 

## Pipeline explanation

The pipeline roughly consists of the following steps

1. * Lidar segmentation* A deep recurrent neural network that takes the lidar point cloud processed in a special way and segments lidar points outputting the probably of each point being an obstacle or not. We've coined this part `lidarnet-segmenter`. 

We've used a novel approach 
![lidarnet segmenter](https://s3.amazonaws.com/team-tea-udacitydidi/segmenter.gif)


